---
title: "Part2"
author: "Danshu"
date: "2023-03-23"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load-data}
df1 <- read.csv("houseprices.csv")
df2 <- read.csv("houseprices2.csv")
```

Load packages:
  
```{r packs, message = FALSE}
# load packages
library(ggplot2)
library(tidyverse)
library(gridExtra)
library(dplyr)
library(countrycode)
library(arm)
library(visdat)
library(patchwork)
library(hrbrthemes)
library(viridis)
library(Metrics)
library(caret)
library(interactions)
library(DescTools)
library(naivebayes)
library(psych)
library(caret)
library(corrplot)
library(glmnet)
library(randomForest)
theme_set(theme_classic())
```

```{r setting}
options(scipen = 999)
```

# Section 1

## Data pre-processing:

```{r pre-processing}
# view missings
vis_miss(df1)
```

```{r 1}
# check detail for each variable in data set
str(df1)
```

```{r 2}
# combine variables which are similar
d1<- df1%>%
  mutate(TotalBath = FullBath + HalfBath)

#replace missings in GarageType since GarageArea = 0
d1["GarageType"][is.na(d1["GarageType"])] <- "No Garage"

# remove real NA
d1 <- na.omit(d1)
```


```{r 3}

chars1 <- colnames(select_if(d1,is.character))

# factor variables
d1[,chars1] <- lapply(d1[,chars1] , factor)

#check detail for each variable in data set after factorization
str(d1)
```

```{r 4}
# drop variables which are not useful 
d2 <- subset(d1, select = -c(Id,FullBath,HalfBath))

lapply(d2[chars1], table)
```

```{r 5}
d2 <- subset(d2, select = -c(Utilities))
```

## Model 1
```{r 6}
# find the index for the type of variables which only have one obsevation
match(c("Mix"),d2$Electrical)
match(c("Floor"),d2$Heating)
```

```{r 7}
# remove the obsevations find above
N <- c(1:398,400:1321,1323:dim(d2)[1])

# linear regression with LOOCV method
preds1 <- numeric(dim(d2)[1]-2)
absolute_error <- numeric(dim(d2)[1]-2)
for (i in N){
  fit1 <- lm(SalePrice~.,data=d2[-i,])
  preds1[i] <- predict(fit1,d2[i,])
  absolute_error[i] <- abs(preds1[i]-d2$SalePrice[i])
  }
summary(fit1)
Mean_error <- sum(absolute_error)/length(N)
```

# Section 2
## Model 2
```{r 8}
##  select three most impact variables
intercept_only <- lm(d2$SalePrice ~ 1, data=d2)
full <- lm(d2$SalePrice ~ ., data=d2)
forward <- stepAIC(intercept_only,  scope=formula(full), steps = 3, direction='forward', trace=F)
forward$anova
```


```{r 9}
d3 <- subset(d2, select = c(SalePrice,OverallQual,GrLivArea,Neighborhood))

# predict performance with three variables
preds2 <- numeric(dim(d3)[1])
absolute_error2 <- numeric(dim(d3)[1])
for (i in 1:dim(d3)[1]){
  fit2 <- lm(SalePrice~.,data=d3[-i,])
  preds2[i] <- predict(fit2,d3[i,])
  absolute_error2[i] <- abs(preds2[i]-d3$SalePrice[i])
  }
summary(fit2)
Mean_error2 <- sum(absolute_error2)/dim(d3)[1]
```

## Model 3

```{r 10}
## more variables
forward2 <- stepAIC(intercept_only, scope=formula(full), direction='forward', trace=F) 
```

```{r 11}
d4 <- forward2$model
names(d4)[names(d4) == "d2$SalePrice"] <- "SalePrice"

# predict performance with the best model
preds3 <- numeric(dim(d4)[1])
absolute_error3 <- numeric(dim(d4)[1])
for (i in 1:dim(d4)[1]){
  fit3 <- lm(SalePrice~.,data=d4[-i,])
  preds3[i] <- predict(fit3,d4[i,])
  absolute_error3[i] <- abs(preds3[i]-d4$SalePrice[i])
  }
summary(fit3)
Mean_error3 <- sum(absolute_error3)/dim(d4)[1]
```

# Section 3
```{r 12, fig.height=3,fig.width=5}
# check assumptions of model in part 1
par(mfrow=c(2,2))
plot(fit1)
```


```{r 13, fig.height=1.5,fig.width=4}
# check the normality of SalePrice
par(mfrow=c(1,2))
qqnorm(d2$SalePrice, ylab = "SalePrice")
qqline(d2$SalePrice)
hist(d2$SalePrice, main = "SalePrice", xlab = "price",breaks = 100)
```

### log-transformation

```{r 14,fig.height=1.5,fig.width=4}
par(mfrow=c(1,2))
# add log
d5 <- d2%>%
  mutate(LogSalePrice = log(SalePrice))
qqnorm(d5$LogSalePrice, ylab = "Log(SalePrice)")
qqline(d5$LogSalePrice)
hist(d5$LogSalePrice, main = "Log(SalePrice)", xlab = "price",breaks = 100)
```

```{r 15}
d5 <- subset(d5, select = -c(SalePrice))

# check performance of the data after log transformation
preds4 <- numeric(dim(d5)[1]-2)
absolute_error4 <- numeric(dim(d5)[1]-2)
for (i in N){
  fit4 <- lm(LogSalePrice~.,data=d5[-i,])
  preds4[i] <- predict(fit4,d5[i,])
  absolute_error4[i] <- abs(exp(preds4[i])-exp(d5$LogSalePrice[i]))
  }
summary(fit4)
Mean_error4 <- sum(absolute_error4)/length(N)
```

# Interaction

```{r 17}
fit5 <-lm(SalePrice~(LotArea + Street + LotShape + LandContour + Neighborhood + BldgType + HouseStyle + OverallQual + OverallCond + YearBuilt + RoofStyle + Foundation + TotalBsmtSF + Heating + CentralAir + Electrical + GrLivArea + BedroomAbvGr + KitchenAbvGr + KitchenQual + TotRmsAbvGrd + Fireplaces + GarageType + GarageArea + MoSold + YrSold + TotalBath)*OverallQual, data = d2)
fit6 <- step(fit5, trace=F)
summary(fit6)
```

```{r interaction-plot}
# interaction between OverallQual and LotShape
interact_plot(fit5, pred = OverallQual, modx = LotShape ) + labs(y ="SalePrice", x = "Overall material and finish quality")
```

### fit model with possible interation

```{r 19}
# perdiction performance of new model
preds5 <- numeric(dim(d2)[1]-2)
absolute_error5 <- numeric(dim(d2)[1]-2)
for (i in N){
  fit8 <- lm(SalePrice~.+ LotShape:OverallQual  ,data=d2[-i,])
  preds5[i] <- predict(fit8,d2[i,])
  absolute_error5[i] <- abs(preds5[i]-d2$SalePrice[i])
  }
summary(fit8)
Mean_error5 <- sum(absolute_error5)/length(N)
```

# Section 4

## Data preprocessing

```{r 20}
# combine variables which are similar
df2<- df2%>%
  mutate(TotalBath = FullBath + HalfBath,
         BsmtTotalBath = BsmtFullBath + BsmtHalfBath)
df2<- subset(df2,select=-c(FullBath,HalfBath,BsmtFullBath,BsmtHalfBath))
```

### Missings

```{r 21}
# check missings for new data
missing <- df2 %>% is.na() %>% colSums()
sort(missing,decreasing = TRUE)
```

#### Categorical vairbales
```{r 22}
# some NA entries in the data sets actually are not missing
# etc. rather than representing a missing value. These values are replaced accordingly

df2$Alley[is.na(df2$Alley)] = "No alley"
df2$BsmtQual[is.na(df2$BsmtQual)] = "No basement"
df2$BsmtCond[is.na(df2$BsmtCond)] = "No basement"
df2$BsmtExposure[is.na(df2$BsmtExposure)] = "No basement"
df2$BsmtFinType1[is.na(df2$BsmtFinType1)] = "No basement"
df2$BsmtFinType2[is.na(df2$BsmtFinType2)] = "No basement"
df2$FireplaceQu[is.na(df2$FireplaceQu)] = "No fireplace"
df2$GarageType[is.na(df2$GarageType)] = "No garage"
df2$GarageFinish[is.na(df2$GarageFinish)] = "No garage"
df2$GarageQual[is.na(df2$GarageQual)] = "No garage"
df2$GarageCond[is.na(df2$GarageCond)] = "No garage"
df2$PoolQC[is.na(df2$PoolQC)] = "No pool"
df2$Fence[is.na(df2$Fence)] = "No fence"
df2$MiscFeature[is.na(df2$MiscFeature)] = "None"
```

#### Numeric variables
```{r 23}
#replace missing values in LotFrontage column in training and test sets with median
df2$LotFrontage[is.na(df2$LotFrontage)] = median(df2$LotFrontage, na.rm = TRUE)

# Unfortunately some of the missing values cannot be reasonably estimated (e.g. the year in which the garage of a house was built)
#replace missing values in GarageYrBlt column in training and test sets with 0
df2$GarageYrBlt[is.na(df2$GarageYrBlt)] = 0
```

#### Actual Missings
```{r 24}
# Remove the values which are missing
df2 <- na.omit(df2)
```

```{r 25}
# Delete ID
df_4 <- subset(df2, select = -c(Id))
# Factorize category variables
chars<-colnames(select_if(df_4, is.character))
df_4[,chars] <- lapply(df_4[,chars], factor)
str(df_4)
```


```{r 26}
lapply(select_if(df_4, is.factor), table)
```

```{r 27}
# Find index of the type of variables which only has one observation
match(c("PosA","RRAe","RRAn"),df_4$Condition2)

match(c("ClyTile","Membran","Metal","Roll"),df_4$RoofMatl)

match(c("AsphShn","CBlock","ImStucc"),df_4$Exterior1st)

match(c("CBlock","Other"),df_4$Exterior2nd)

match(c("Po"),df_4$ExterCond)

match(c("Floor"),df_4$Heating)

match(c("Po"),df_4$HeatingQC)

match(c("Mix"),df_4$Electrical)

match(c("Sev"),df_4$Functional)

match(c("TenC"),df_4$MiscFeature)

# delete Utilities
df_4 <- subset(df_4,select = c(-Utilities))
# positons   
drop_num <- c(582,1225,998,1291,271,121,1269,1006,1363,1182,1363,594,250,1314,325,398,664,1378)
```

## simple linear regression

```{r 28}
# simple linear regression
N4 <- 1:dim(df_4)[1]
N4 <- N4[-drop_num]
# check performance
pred4_1 <- numeric(dim(df_4)[1]-18)
ae4_1 <- numeric(dim(df_4)[1]-18)
for (i in N4){
  fit4_1 <- lm(SalePrice~., data=df_4[-i,])
  pred4_1[i] <- predict(fit4_1, df_4[i,])
  ae4_1[i] <- abs(pred4_1[i]-df_4$SalePrice[i])
  }
summary(fit4_1)
ME4_1 <- sum(ae4_1)/length(N4)
```
## Variable selection model
```{r 29}
# forward selection
intercepts <- lm(df_4$SalePrice ~ 1, data=df_4)
full4 <- lm(df_4$SalePrice ~ ., data=df_4)
forward4 <- stepAIC(intercepts,  scope=formula(full4), direction='forward', trace=F)
# data with selected variables
df_41 <- forward4$model
forward4$anova
```

```{r 30}
# rename new data
names(df_41)[names(df_41) == "df_4$SalePrice"] <- "SalePrice"
# check number of obsevations in each type
lapply(select_if(df_41, is.factor), table)
```

```{r 31}
# find index 
match(c("PosA","RRAe","RRAn"),df_41$Condition2)

match(c("ClyTile","Membran","Metal","Roll"),df_41$RoofMatl)

match(c("AsphShn","CBlock","ImStucc"),df_41$Exterior1st)

match(c("Sev"),df_41$Functional)

# positions
drop_num2 <- c(582,1225,998,1291,271,121,1269,1006,1363,1182,664)


```

```{r 32}
# variable selection
N4_2 <- 1:dim(df_41)[1]
N4_2 <- N4_2[-drop_num2]

# perfomance(LOOCV)
pred4_2 <- numeric(dim(df_41)[1]-11)
ae4_2 <- numeric(dim(df_41)[1]-11)
for (i in N4_2){
  fit4_2 <- lm(SalePrice~.,data=df_41[-i,])
  pred4_2[i] <- predict(fit4_2,df_41[i,])
  ae4_2[i] <- abs(pred4_2[i]-df_41$SalePrice[i])
  }
summary(fit4_2)
ME4_2 <- sum(ae4_2)/length(N4_2)
```

## random forest
```{r 33}
# peridict and check performance
pred4_3 <- numeric(dim(df_4)[1]-18)
ae4_3 <- numeric(dim(df_4)[1]-18)
for (i in N4){
  set.seed(1)
  fit4_3 <- randomForest(SalePrice~., data=df_4[-i,],ntree=100)
  pred4_3[i] <- predict(fit4_3, df_4[i,])
  ae4_3[i] <- abs(pred4_3[i]-df_4$SalePrice[i])
  }
summary(fit4_3)
ME4_3 <- sum(ae4_3)/length(N4)
```
### Lasso Model
```{r}
# peridict and check performance
pred4_4 <- numeric(dim(df_4)[1]-18)
ae4_4 <- numeric(dim(df_4)[1]-18)
for (i in N4){
  y_train <- df_4$SalePrice[-i]
  x_train <- data.matrix(df_4[-i,-75])
  #perform k-fold cross-validation to find optimal lambda value
  cv_model <- cv.glmnet(x_train, y_train, alpha = 1, nfolds = 10)
  best_lambda <- cv_model$lambda.min
  fit4_4 <- glmnet(x_train, y_train, alpha = 1, standardize = TRUE, lambda = best_lambda)
  pred4_4[i] <- predict(fit4_4,s = best_lambda, newx = data.matrix(df_4[i,-75]))
  ae4_4[i] <- abs(pred4_4[i]-df_4$SalePrice[i])
  }
summary(fit4_4)
ME4_4 <- sum(ae4_4)/length(N4)
```

### Analysis Plots

```{r impact-vars}
##  select three most impact variables
# forward selection
intercepts5 <- lm(df_4$SalePrice ~ 1, data=df_4)
full5 <- lm(df_4$SalePrice ~ ., data=df_4)
forward5 <- stepAIC(intercepts5,  scope=formula(full5), steps = 3, direction='forward', trace=F)
# data with selected variables
df <- forward5$model
df
```

```{r p1}
ggplot(d1,aes(x=factor(0),SalePrice))+geom_boxplot(width=0.5,fill = "#F4EDCA", color = "#000000")+
    scale_x_discrete(breaks = NULL) +
    xlab(NULL)+
  theme_minimal() + coord_flip() +theme(axis.text=element_text(size=12),
        axis.title=element_text(size=14,face="bold"))
```

```{r LotFrontage-plot}
ggplot(df2,aes(x=factor(0),LotFrontage))+geom_boxplot(width=0.5,fill = "#F4EDCA", color = "#000000")+
    scale_x_discrete(breaks = NULL) +
    xlab(NULL)+
  theme_minimal() + coord_flip() +theme(axis.text=element_text(size=12),
        axis.title=element_text(size=14,face="bold"))

hist(df2$LotFrontage, main = "LotFrontage", xlab = "feet ", breaks = 100)
```

```{r p2}
ggplot(df_41, aes(x = factor(OverallQual), y = SalePrice, fill = as.factor(OverallQual)))+
  geom_boxplot(show.legend = FALSE)+
  theme_minimal()+ 
  theme(axis.text=element_text(size=12),axis.title=element_text(size=14,face="bold"))+
  scale_fill_brewer(palette="Set3")+
  xlab("Overall Quality")

ggplot(df_4, aes(x= GrLivArea, y= SalePrice))+
  geom_point()+
  theme_minimal()+ 
  theme(axis.text=element_text(size=12),axis.title=element_text(size=14,face="bold"))+
  scale_fill_brewer(palette="Set3")+xlab("Above ground living area (square feet)")
```




